<?xml version="1.0" encoding="UTF-8"?>

<section xml:id="sec-markov-chains" xmlns:xi="http://www.w3.org/2001/XInclude">
  <title>Markov chains</title>

<p>
We will introduce the concept of a Markov chain by example.
</p>


<example xml:id="ex-markov">
<statement>
<p>
Suppose there are two chains of grocery stores, <m>W</m> and <m>C</m>, and that 
each month
<ul>
<li>
<p>90\% of people that shop at <m>W</m> continue to shop at <m>W</m> in the next month,
</p>
</li>
<li>
<p>10\% of people that shop at <m>W</m> will switch to <m>C</m> in the next month,
</p>
</li>
<li>
<p>70\% of people that shop at <m>C</m> continue to shop at <m>C</m> in the next month, and
</p>
</li>
<li>
<p>30\% of people that shop at <m>C</m> will switch to <m>W</m> in the next month.
</p>
</li>
</ul>
This is represented graphically in <xref ref="fig-grocery"/>.
</p>

<figure xml:id="fig-grocery">
  <caption>
    Diagram representing the probability of where a person will shop next month (tip of the arrow),
    depending on where they shopped this month (tail of the arrow).
  </caption>
  <image width="60%">
    <latex-image>
\begin{tikzcd}[column sep=5em]
  W \amp\amp C
  \arrow["0.9", from=1-1, to=1-1, loop, in=145, out=215, distance=10mm]
  \arrow["0.1", from=1-1, to=1-3, bend left=20]
  \arrow["0.3"', from=1-3, to=1-1, bend left=20]
  \arrow["0.7", from=1-3, to=1-3, loop, in=325, out=35, distance=10mm]
\end{tikzcd}
    </latex-image>
  </image>
</figure>

<p>
Suppose that initially 100 people shop at <m>W</m> and <m>80</m> people shop at 
<m>C</m>.
</p>

<p>
<term>Question:</term> How many people shop at <m>W</m> and <m>C</m> in the following 
month?
</p>
<p>
For short, we will say that a person is in <em>state</em> <m>W</m> if they shop 
at <m>W</m> that month, and in <em>state</em> <m>C</m> if they shop at <m>C</m>. Then
<md>
<mrow>
\amp \# \text{ of people in state <m>W</m> next month} \\
\amp  \qquad\qquad = 0.9(\text{# of <m>W</m> customers}) + 0.3(\text{<m>#</m> of <m>C</m> customers}) \\
\amp  \qquad\qquad = 0.9(100) + 0.3(80)\\
\amp  \qquad\qquad = 90 + 24 \\
\amp  \qquad\qquad = 114,
</mrow>
</md>
and
<md>
<mrow>
\amp \#\text{ of people in state <m>C</m> next month} \\ 
\amp \qquad\qquad = 0.1(\text{<m>#</m> of <m>W</m> customers}) + 0.7(\text{<m>#</m> of <m>C</m> customers}) \\
\amp \qquad\qquad  = 10 + 56 \\
\amp \qquad\qquad = 66
</mrow>
</md>,
which answers the question. Note that
<me>
\begin{bmatrix}
  0.9 \amp  0.3 \\ 0.1 \amp  0.7
\end{bmatrix}
\colvec{100}{80} = \colvec{114}{66},
</me>
so if <m>\vect{x}_0 = \colvec{100}{80}</m> is the vector encoding the initial states 
(the amount of customers in the current month), then
<me>
\vect{x}_1 = P\vect{x}_0, \qquad \text{where } P =
\begin{bmatrix}
  0.9 \amp  0.3 \\ 0.1 \amp  0.7
\end{bmatrix}
</me>
is the vector encoding the states after 1 month (the next month).
</p>
</statement>
</example>

<p>
The important thing about this model is that what happens in the next month only 
depends on the states in the current month. This abstracts into the idea of a 
Markov chain.
</p>


<definition xml:id="def-markov-chain">
<statement>
<p>
A <term>Markov chain</term> is a process that changes over time. It consists of:
<ul>
<li>
<p>A finite number of <term>states</term>. At each step, the process can be in any 
one of the states.
</p>
</li>
<li>
<p>
At the next step, the process can either stay in the same state, or switch to a 
different state.
</p>
</li>
<li>
<p>
The probability of moving from state <m>j</m> to state <m>i</m> is constant. 
At any step, this probability only depends on the two states <m>j</m> and <m>i</m>.
</p>
</li>
</ul></p>
</statement>
</definition>

<remark xml:id="rem-discrete-time-markov">
<p>
This is sometimes called a discrete time Markov chain, because the time steps 
(the intervals at which the process changes) are not continuous.
</p>
</remark>

<p>
<term>Notation and terminology:</term>
<ul>
<li>
<p>We will use <m>1, 2, \dots, n</m> to label the states.</p>
</li>
<li>
<p>We write <m>\vect{x}_k</m> for the vector whose <m>i</m>th entry is the number 
of members (people/objects etc.) in state <m>i</m> after <m>k</m> steps. We call 
<m>\vect{x}_k</m> a <term>state vector</term>. So <m>\vect{x}_0</m> is the 
initial data, called the <term>initial state vector</term>, and <m>\vect{x}_1</m> 
is the data after 1 step, <m>\vect{x}_2</m> is the data after 2 steps, and so on.</p>
</li>
<li>
<p>We write <m>p_{ij}</m> for the probability of moving from state <m>j</m> to 
state <m>i</m> in 1 step, called a <term>transition probability</term>.</p>
</li>
<li>
<p>The <term>transition matrix</term> is <m>P = [p_{ij}]</m>, where <m>p_{ij}</m> 
are the transition probabilities
</p>
</li>
</ul>
</p>

<remark xml:id="rem-probs-and-indices">
<p>
Warning: do not confuse <m>p_{ij}</m> with the probability of moving from 
state <m>i</m> to state <m>j</m>, which is <m>p_{ji}</m>. We write it this way 
to make the formulas neater later.
</p>
</remark>

<p>
For all <m>i</m> and <m>j</m>, we have <m>0 \le p_{ij} \le 1</m>, since they 
are probabilities. And
<me>
p_{11} + p_{12} + \dots + p_{1n} = 1,
</me>
because this is the probability of moving to 1 from 1, or to 1 from 2, <m>\dots</m>, 
or to 1 from <m>n</m>, and state 1 must move to one of the <m>n</m> states. 
Therefore the first column of <m>P</m> sums to 1. Similarly, each of the other 
columns sum to 1, <em>so the transition matrix <m>P</m> is stochastic</em>. 
(Check this in <xref ref="ex-markov"/>.)
</p>

<example xml:id="ex-markov-setup">
<statement>
<p>
In <xref ref="ex-markov"/>, let state 1 be <m>W</m> and state 2 be <m>C</m>. Then 
the initial state vector is
<me>
\vect{x}_0 = \colvec{100}{80}
</me>
and the transition matrix is
<me>
P =
\begin{bmatrix}
  0.9 \amp  0.3 \\
  0.1 \amp  0.7
\end{bmatrix}
</me>.
After <m>1</m> month, <m>\vect{x}_1 = P\vect{x}_0</m>. After <m>2</m> months, 
<m>\vect{x}_2 = P\vect{x}_1 = P(P\vect{x}_0) = P^2\vect{x}_0</m>. So after 
<m>k</m> months,
<me>
\vect{x}_k = P\vect{x}_{k-1} = \cdots = P^k\vect{x}_0.
</me>
</p>
</statement>
</example>

<assemblage xml:id="transition-probs">
<p>
<term>Fact:</term> if <m>P</m> is the transition matrix of a Markov chain, then 
the probability of moving from state <m>j</m> to state <m>i</m> in <m>k</m> steps 
is the <m>(i, j)</m>-entry of <m>P^k</m>.
</p>
</assemblage>

<example xml:id="ex-finding-markov-probs">
<statement>
<p>
In <xref ref="ex-markov"/>, what is the probability that someone who shops at 
<m>W</m> this month will shop at <m>C</m> in two months time?
</p>
<p>This is the probability of moving from state 1 to state 2 in 2 steps, which 
is the <m>(2, 1)</m>-entry of <m>P^2</m>. We have
<me>
P^2 =
\begin{bmatrix}
  0.9 \amp  0.3 \\
  0.1 \amp  0.7
\end{bmatrix}
\begin{bmatrix}
  0.9 \amp  0.3 \\
  0.1 \amp  0.7
\end{bmatrix}
=
\begin{bmatrix}
  0.84 \amp  0.48 \\
  0.16 \amp  0.52
\end{bmatrix}
,
</me>
which has <m>(2, 1)</m>-entry 0.16. Therefore the probability is 0.16.
</p>
</statement>
</example>

</section>